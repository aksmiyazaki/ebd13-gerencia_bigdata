%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
\usepackage[portuguese]{babel}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\input{structure.tex} % Include the file specifying the document structure and custom commands
\usepackage{listings}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{EBD13 Gerência de Dados e Computação em Nuvem} % Title of the assignment

\author{Alexandre Miyazaki\\ \texttt{aksmiyazaki@gmail.com}} % Author name and email address

\date{Universidade Federal do Rio Grande do Sul} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\section{Introdução e Objetivos}
Devido ao escopo flexível do trabalho, ele será executado em Python, na \emph{Google Cloud}. O trabalho de EBD09 foi executado em R neste mesmo ambiente, e o Exercício 1 de EBD13 foi executado na \emph{Amazon} em Python, portanto, ao final desse trabalho será possível fazer uma comparação entre deploy do ambiente Python nessas Clouds, além de destacar pontos positivos e negativos das linguagens citadas trabalhando com o Spark.

O principal objetivo do trabalho é chegar no mesmo ponto do trabalho de EBD09, ou seja, realizar algumas análises exploratórias plotando gráficos. Se for possível, é desejado chegar até a geração de um modelo, mesmo que pouco performático, de \emph{Machine Learning}.

Além disso, antes de começar, será realizada uma análise simplificada de arquiteturas para definirmos a melhor a ser utilizada. Essa análise visa a escolha do melhor ambiente disponível para que o trabalho seja realizado.

\subsection{Organização do Texto}
O texto das próximas seções está organizado da seguinte forma:

\begin{itemize}
    \item A Seção \ref{sect:data} disserta sobre os dados utilizados;
    \item A Seção \ref{sect:infra} fala sobre a Infraestrutura e Deploy de ambiente;
    \item A Seção \ref{sect:expl_anal} faz a análise dos dados;
    \item A Seção \ref{sect:critic_anal} faz uma análise crítica e comparações de ferramentas e linguagens;
    \item A Seção \ref{sect:end} conclui o trabalho;
    \item A Seção \ref{sect:eval_data} possui informações complementares.
\end{itemize}

\newpage
\section{Descrição do Dataset}
\label{sect:data}

Os dados utilizados foram os mesmos do trabalho de EBD09. As \emph{Features} do dataset não serão detalhadas, vamos ir detalhando apenas aquelas utilizadas na análise exploratória. Algumas características dos dados podem ser observadas na Tabela \ref{tab:data_data}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Tamanho & Formato & Número de Features &  Número de Observações \\ \hline \hline
5.554 MB & CSV & 166 & 8.627.367 \\ \hline
\end{tabular}
\caption{Descrição dos Dados.}
\label{tab:data_data}
\end{table}

Para realizar todas as tarefas, foi submetido o dataset para o HDFS. O caminho completo do arquivo foi obtido navegando pelo sistema, conforme mostra a Figura \ref{fig:data_on_hdfs}.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/file_hdfs.png}
  \caption{Arquivos no HDFS, identificado como \emph{enem\_2016.csv}.}
  \label{fig:data_on_hdfs}
\end{figure}

\newpage
\section{Infraestrutura e Deploy de Ambiente}
\label{sect:infra}

Como base da infraestrutura, utilizou-se a parte de \emph{Data Proc} da Google Cloud. Inicialmente, criou-se um cluster com 1 \emph{master} e 2 \emph{slaves} apenas para a realização de um deploy inicial. A configuração dessas máquinas não necessariamente é importante pois só iremos setar e testar o ambiente.

\subsection{Deploy de Ambiente}

Para fazer a avaliação da arquitetura, foram realizados testes de deploy do ambiente (era necessário conhecer como ele era realizado para replicar nas análises de infraestrutura). Felizmente a implantação das aplicações necessárias precisa apenas de uma ação: colocar o script \emph{gs://dataproc-initialization-actions/jupyter/jupyter.sh} como ação de inicialização.

Com isso, o ambiente montado já vem com Hadoop, Spark, PySpark, Jupyter e Anaconda, todos configurados. O último vem com um problema de configuração que não permite que sejam instalados novos pacotes. Isso foi resolvido executando o comando abaixo. Após esse comando, foi possível instalar pacotes normalmente (embora eu tenha dúvidas se essa é a melhor abordagem para a solução).

\begin{lstlisting}[caption= {Solução de problema de Permissão.},captionpos=b]
sudo chown -R $USER /opt/conda
\end{lstlisting}

\subsection{Avaliação de Arquitetura Ideal}

Antes de começar a efetivamente fazer a análise dos dados, decidi por avaliar algumas opções de arquitetura e ver qual seria a melhor opção (visto que, no momento, não estou preocupado com custo, apenas com desempenho). As arquiteturas testadas podem ser visualizadas na tabela \ref{tab:arqs}. Importante salientar que todas as arquiteturas possuem apenas um nó Master, cuja configuração é sempre igual a dos Slaves (a salvo na configuração 4, onde ele possui 2vCPU, 7.5GB de memória e 50 GB SSD).

Houveram problemas de alocação de arquitetura, pois há restrições no plano free da \emph{Google Cloud}. Conseguimos montar um cluster com no máximo 8 vCPUs e com no máximo 100GB (somatório total) de SSD. Portanto, analisei o que era possível ser criado.

A avaliação foi realizada inicialmente, apenas carregando os dados, com o trecho de código abaixo. Para cada execução, foi reiniciado o Kernel Python.

\begin{lstlisting}[caption= {Carga de dados no Spark},captionpos=b, language=python]
start = time.time()

df = sqlContext.read.option("delimiter", ';')
.load('hdfs:///user/aksmiyazaki/enem_2016.csv', 
                      format='com.databricks.spark.csv', 
                      header='true', 
                      inferSchema='true')

end = time.time()
t = end - start
\end{lstlisting}


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Identificador & Número de Slaves & vCPUs &  Memória & Disco \\ \hline \hline
1 & 2 & 2 & 7,5 GB & 200GB HD  \\ \hline
2 & 2 & 2 & 7.5 GB & 30GB SSD \\ \hline
3 & 3 & 2 & 7.5 GB & 20GB SSD  slaves. \\ \hline
4 & 6 & 1 (VCPUj) & 3.75 GB & 100GB HD   \\ \hline
\end{tabular}
\caption{Descrição de Arquiteturas testadas.}
\label{tab:arqs}
\end{table}

Após a carga, foi criada uma \emph{lazily evaluated view} (trecho de código abaixo). Essa tabela pode ser consultada com SparkSQL. 

\begin{lstlisting}[caption= {Criação da Tabela para Consulta},captionpos=b, language=python]
df.createOrReplaceTempView("table_enem2016")
\end{lstlisting}

Com a tabela criada, os dados foram consultados utilizando o código abaixo. Essa consulta foi usada pois é necessário passar por todo o volume de dados para calcular a média de uma coluna (pelo menos por toda a coluna). Como os dados são distribuídos, mesmo sendo uma consulta simples, é uma forma adequada de fazer a avaliação. É importante salientar que nesse ponto o Kernel não é mais reinicializado.

\begin{lstlisting}[caption= {Consulta Simplificada para Avaliação},captionpos=b, language=python]
start = time.time()

df2 = spark.sql("Select mean(NU_NOTA_CN) from table_enem2016")
df2.collect()\

end = time.time()
\end{lstlisting}

Os resultados da avaliação (média de tempos) podem ser visualizados na Tabela \ref{tab:mean_times}. Para maiores detalhes, consulte a Seção \ref{sect:eval_data}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Configuração & Carga de Dados (média) &  Execução de  Consulta Simples (média) \\ \hline 
\hline
1 & 184.79 &  57.83
\\ \hline
2 & 172.98 & 49.72
 \\ \hline
3 & 124.81
 &  40.24
 \\ \hline
4 & 100.57
 &  27.92
 \\ \hline
\end{tabular}
\caption{Tempos médios de execução, em \textbf{segundos}.}
\label{tab:mean_times}
\end{table}

Algumas considerações sobre a análise:

\begin{itemize}
    \item Esperava que a configuração 2 tivesse um ganho bem maior em relação a 1, pois estamos usando SSD. Não cheguei a aprofundar a análise do motivo desse comportamento;
    \item A configuração 4 foi aquela que performou melhor. Mesmo a configuração anterior sendo equivalente em número de cores e armazenamento em SSD, muito provavelmente o gargalo é leitura de disco e nesse caso, tendo os dados distribuídos em mais workers acabou sendo mais impactante;
    \item Possívelmente haveriam outras configurações melhores, se não tivéssemos limites de alocação.
\end{itemize}

Portanto, a arquitetura de número 4 é a mais performática em termos de processamento e é a que iremos utilizar no trabalho. Ela não é tão adequada para o processamento de grandes volumes de dados (em uma consulta simples, levando em torno de 27 segundos), mas é a melhor que conseguimos montar no ambiente disponibilizado. É importante salientar também que essa configuração pode não ser o melhor custo benefício (isso precisaria ser levado em consideração no mundo real).

A Figura \ref{fig:hdfs_data} mostra as estatísticas do master e de um dos datanodes (slaves) após a carga de dados. Pode-se notar que a arquitetura utilizada possui 6 slaves pela linha que reporta o número de Live datanodes.

\begin{figure}[H]
\centering
  \includegraphics[width=0.6 \linewidth]{img/hdfs-occupation.png}
  \caption{Estatísticas do HDFS.}
  \label{fig:hdfs_data}
\end{figure}

Para ficar mais claro a arquitetura utilizada a Figura \ref{fig:arch} mostra sua representação. É importante notar que o Master possui configuração diferente dos Slaves.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8 \linewidth]{img/arch.png}
  \caption{Arquitetura utilizada no trabalho.}
  \label{fig:arch}
\end{figure}

\newpage
\section{Análise Exploratória dos Dados}
\label{sect:expl_anal}

A análise dos dados tem como principal objetivo identificar \emph{Features} que tenham relação com as notas dos candidatos. Para isso, irei realizando inferências a medida que a análise avançar, exibindo os resultados obtidos.

A primeira intuição é avaliar se as regiões do País afetam as médias dos candidatos. Para isso, executou-se uma consulta , que calculava também a média total, gerando o gráfico da Figura \ref{fig:avg_uf}.

\begin{figure}[H]
\centering
  \includegraphics[width= 0.8 \linewidth]{img/mean_by_uf.png}
  \caption{Médias por UF.}
  \label{fig:fig_maq_criad}
\end{figure}

É visível a discrepância de notas da parte sudeste do país com o restante. Portanto, concluí-se que essa é uma \emph{Feature} bem significativa para inferir a nota do candidato.

Fazendo uma análise da idade relacionada a média total, podemos ver uma tendência decrescente nos dados a medida que a idade aumenta, na Figura \ref{fig:mean_by_age}. Analisando a correlação de Idade com a média total, temos um valor de -0.52, o que indica uma correlação negativa razoável (a medida que aumenta a idade, a média cai, o que confirma a análise visual).

\begin{figure}[H]
\centering
  \includegraphics[width=0.8 \linewidth]{img/mean_by_age.png}
  \caption{Médias por Idade.}
  \label{fig:mean_by_age}
\end{figure}

Também foi realizada uma análise, verificando a renda familiar (informada na Feature Q006). A renda é representada por letras, onde A indica sem renda e Q indica mais de R\$ 17.600,00 (ordem crescente). Na Figura \ref{fig:mean_by_wage} é visível a tendência crescente da média total a medida que a renda aumenta. É possível ver grandes discrepâncias nas notas de Redação e Matemática. Com isso, concluí-se que essa é uma Feature que também tem relação com a performance dos candidatos.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8 \linewidth]{img/mean_by_wage.png}
  \caption{Médias por Faixa de Renda.}
  \label{fig:mean_by_wage}
\end{figure}

Também foi realizada uma análise por sexo, cujo resultado pode ser visualizado na Figura \ref{tab:mean_by_sex}. Notar que a diferença no Score total não foi significativa.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8 \linewidth]{img/mean_by_sex.png}
  \caption{Médias por Sexo.}
  \label{fig:mean_by_sex}
\end{figure}

Analisando o estado civil dos candidatos, a Figura \ref{fig:mean_by_civil} mostra os resultados. Há uma clara relação deles com o score total (solteiros tiram notas maiores que os demais estados). É importante salientar que esses resultados podem estar correlacionados com Idade.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8 \linewidth]{img/mean_by_civil.png}
  \caption{Médias por Estado Civil.}
  \label{fig:mean_by_sex}
\end{figure}

Ao analisar mais um campo, o de Raça, identificamos que há dois valores que não agregam tanta informação assim (Não Declarado e Não consta Informação). Há em torno de 165.000 registros com essas informações, todavia, nosso dataset possui mais de 8 milhões. Portanto, decidiu-se por avaliar assim mesmo, o resultado pode ser visualizado na Figura \ref{fig:mean_by_raca}. É possível notar que pessoas Brancas têm uma média superior as demais raças e, portanto, iremos considerar esse campo.

\begin{figure}[H]
\centering
  \includegraphics[width=0.8 \linewidth]{img/mean_by_raca.png}
  \caption{Médias por Raça declarada.}
  \label{fig:mean_by_raca}
\end{figure}


\subsection{Modelo de Regressão}

Com algumas Features selecionadas, foi decidido por tentar realizar o treinamento de um modelo de \emph{Machine Learning} para tentar prever as notas de novos candidatos. Para isso, foi selecionado o modelo mais simples de regressão (Linear), utilizando o LinearRegression do PySpark.

Há algumas particularidades do PySpark, relacionando-o com as bibliotecas geralmente utilizadas para machine learning em Python, por exemplo, o fato de todas as features precisarem estar em apenas uma coluna e não trabalhar valores String. Por isso, foi necessário uma série de transformações nos dados antes de começar a treinar o modelo.

Inicialmente, transformou-se os dados representados em strings para vetores de inteiros, com o código abaixo.
Isso foi executado no campo \emph{UF} e no campo \emph{Renda}.

\begin{lstlisting}[caption= {Transformação de dados String.},captionpos=b, language=python]
tokenizer = Tokenizer(inputCol='UF', outputCol="uf_text")
df_text_token = tokenizer.transform(ml_df)

hashingTF = HashingTF(inputCol="uf_text", outputCol="uf_feature",
numFeatures=20)
ml_df = hashingTF.transform(df_text_token)
\end{lstlisting}

Com isso, separou-se os dados em um dataset de treino (70\% dos dados) e teste (30\% dos dados) e realizou-se o treinamento de um regressor linear (código abaixo). O resultado não foi muito bom, as estatísticas dele podem ser visualizadas na Tabela \ref{tab:model_traign}.


\begin{lstlisting}[caption= {Treinamento de modelo de Regressão Linear.},captionpos=b, language=python]
splits = vhouse_df.randomSplit([0.7, 0.3])
train_df = splits[0]
test_df = splits[1]

lr = LinearRegression(featuresCol = 'features', labelCol='Score',
maxIter=10, regParam=0.3, elasticNetParam=0.8)
lr_model = lr.fit(train_df)
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Dataset & Estatística & Valor \\ \hline \hline
Treino & R Squared & 0.21\\ \hline
Teste & R Squared & 0.21 \\ \hline
\end{tabular}
\caption{R Squared do modelo com os Datasets de treino e teste.}
\label{tab:model_traign}
\end{table}

O modelo nitidamente não está bom, o valor de R Squared em 0.21 indica que o modelo consegue prever apenas 21\% da variabilidade em \emph{Score} pode ser explicada usando o modelo. Para melhorar isso, pode-se realizar diversas ações:

\begin{itemize}
    \item Aumentar o conjunto de features;
    \item Mudar o formato de representação de features para se adequarem melhor aos modelos de ML;
    \item Mudar o algoritmo de ML;
    \item Tunar parâmetros do algoritmo de ML;
    \item Realizar uma análise de correlação das features, utilizando um conjunto com base não só na análise visual.
\end{itemize}

De qualquer forma, a experiência adquirida ao treinar um modelo de forma distribuída com o PySpark foi ótima. Optou-se por não realizar a otimização do modelo gerado, por questão de escopo do trabalho.

\newpage
\section{Análise Crítica}
\label{sect:critic_anal}

Sobre a infraestrutura, ela permitiu que o trabalho fosse realizado. As consultas, via de regra, demoravam pouco tempo para responder (menos de 1 minuto cada). Pontos positivos de realizar o trabalho na \emph{Google Cloud} utilizando Python:

\begin{itemize}
    \item Fácil deploy de infraestrutura;
    \item Fácil utilização do ambiente (conexão com Jupyter, execução de scripts).
\end{itemize}

Obviamente que não há só pontos positivos. Devido ao costume de trabalhar com determinadas bibliotecas de Machine Learning com Python, trabalhar com a PySpark pode ser um desafio. Isso porque, ela exige algumas restrições únicas, acredito que por ser montada sobre Scala. Para destacar alguns pontos:

\begin{itemize}
    \item Não trabalha com Strings;
    \item Features precisam estar em uma única coluna do Dataframe.
\end{itemize}

A solução em núvem, em geral, é adequada ao processamento de dados desse nível de volume. É um volume mediano de dados (em torno de 5GB) e por isso, não houve dificuldades de performance. Ao comparar a facilidade de uso do ambiente da Google em relação ao da Amazon (usado no primeiro trabalho), a diferença é muito grande. Na Amazon, houveram problemas de liberação de portas, instalação de pacotes, configuração do Hadoop e Spark, coisas que foram praticamente inexistentes ao fazer o trabalho na Google Cloud. Obviamente que na Amazon há imagens pagas e que talvez facilitem o trabalho a ser realizado, mas a experiência melhor durante os trabalhos, foi com a Google.

\newpage
\section{Conclusão}
\label{sect:end}

A experiência com múltiplas plataformas de Cloud é algo essencial para um curso de Big Data. Isso não deixou a desejar durante as disciplinas EBD09 e EBD13, consegui utilizar dois dos ambientes mais populares e adquirir uma certa familiaridade com eles. 

Sobre o trabalho, inicialmente o objetivo era apenas fazer análises exploratórias (pois nunca havia feito o deploy de Python na Google). Todavia, devido a facilidades de ambiente e obviamente ao prazo do trabalho, consegui evoluir até a montagem de um modelo de ML (mesmo performando mal, todo o processo de geração dele foi coberto).

Acredito que tenha cumprido todos os pontos do escopo do trabalho. Como atividades futuras, pode-se melhorar o modelo gerado, seguindo abordagens citadas nesse próprio trabalho.


\newpage
\section*{Apêndice}
\label{sect:eval_data}
\subsection*{Execução de consultas por Arquitetura}

É importante notar que os identificadores citados nas Tabelas \ref{tab:times_load} e \ref{tab:query} se referem àqueles citados na Tabela \ref{tab:arqs}. A abordagem de avaliação foi bem simplificada, tendo em vista que fazer uma real análise de desempenho não faz parte do escopo do trabalho.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Identificador da Arquitetura & T1 & T2 & T3\\ \hline 
\hline
1 &  187.68613958358765 & 178.42478108406067 & 188.2514045238495\\ \hline
2 & 172.11613035202026 & 172.5200114250183 & 174.2983114719391 \\ \hline
3 & 127.42551612854
 & 121.673460960388
& 125.33324432373
\\ \hline
4 & 106.646942853928
 & 97.2526772022247
& 97.7907633781433
\\ \hline
\end{tabular}
\caption{Tempos de Carga, em \textbf{segundos}.}
\label{tab:times_load}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Identificador da Arquitetura & T1 & T2 & T3\\ \hline 
\hline
1 & 63.009117603302 & 61.09515404701233 & 49.37240242958069\\ \hline
2 & 58.08870315551758 & 46.74635648727417 & 44.3299551010132
\\ \hline
3 & 43.8593015670776
 & 44.337733745575
& 32.537876367569
\\ \hline
4 & 34.5449047088623
 & 24.7415041923523
& 24.4780607223511
\\ \hline
\end{tabular}
\caption{Tempos de Consulta, em \textbf{segundos}.}
\label{tab:query}
\end{table}



\end{document}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/machines_created.png}
  \caption{Máquinas criadas na \emph{cloud}.}
  \label{fig:fig_maq_criad}
\end{figure}

