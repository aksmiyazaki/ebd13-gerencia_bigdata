%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
\usepackage[portuguese]{babel}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\input{structure.tex} % Include the file specifying the document structure and custom commands
\usepackage{listings}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{EBD13 Gerência de Dados e Computação em Nuvem} % Title of the assignment

\author{Alexandre Miyazaki\\ \texttt{aksmiyazaki@gmail.com}} % Author name and email address

\date{Universidade Federal do Rio Grande do Sul} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\section{Introdução}
O objetivo do trabalho é analisar o Dataset do ENEM, assim como foi realizado o trabalho de EBD09. Todavia, agora usaremos Python e PySpark, fazendo o deploy de todo o ambiente na Google Cloud.

O objetivo é analisar o Dataset, além de comparar as facilidades e dificuldades de utilizar Python e R. Além disso, se houver tempo hábil, irei realizar também uma análise de desempenho simplificada, para determinar um mínimo local de arquitetura (número de workers) para executar as análises.


\subsection{Objetivo do Trabaho}

\subsection{Organização do Texto}

\newpage
\section{Dados}

Os dados utilizados foram os mesmos do trabalho de EBD09. Não irei entrar em detalhes de quantas \emph{features} existem nele, vamos fazer uma avaliação a medida que formos avançando na avaliação exploratória. Algumas características dos dados podem ser observadas na Tabela \ref{tab:data_data}.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Tamanho & Formato & Número de Features &  Número de Observações \\ \hline \hline
5.554 MB & CSV & 166 & 8.627.367 \\ \hline
\end{tabular}
\caption{Descrição dos Dados.}
\label{tab:data_data}
\end{table}

Para realizar todas as tarefas, foi submetido o dataset para o HDFS. O caminho completo do arquivo foi obtido navegando pelo sistema, conforme mostra a Figura \ref{fig:data_on_hdfs}.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/file_hdfs.png}
  \caption{Arquivos no HDFS, identificado como \emph{enem\_2016.csv}.}
  \label{fig:data_on_hdfs}
\end{figure}

\newpage
\section{Infraestrutura e Deploy de Ambiente}

Como base da infraestrutura, utilizou-se a parte de \emph{Data Proc} do Google Cloud. Inicialmente, criou-se um cluster com 1 \emph{master} e 2 \emph{slaves} apenas para a realização de um deploy inicial de todo o ambiente. A configuração dessas máquinas não necessariamente é importante pois só iremos setar o ambiente e testá-lo.

\subsection{Deploy de Aplicações}

A \emph{Google Cloud Dataproc} faz com que o ambiente precise de quase nenhuma modificação. O único processo que precisa ser realizado além de lançar o Cluster no Console é colocar o script \emph{gs://dataproc-initialization-actions/jupyter/jupyter.sh} como ação de inicialização.

Com isso, o ambiente montado já vem com Hadoop, Spark, PySpark, Jupyter e Anaconda. O último vem com um problema de configuração que não permite que sejam instalados novos pacotes. Isso foi resolvido executando o comando abaixo. Após esse comando, foi possível instalar pacotes normalmente (embora eu tenha dúvidas se essa é a melhor abordagem para a solução.

\begin{lstlisting}[caption= {Solução de problema de Permissão.},captionpos=b]
sudo chown -R $USER /opt/conda
\end{lstlisting}

\subsection{Avaliação de Arquitetura Ideal}

Antes de começar a efetivamente fazer a análise, decidi por avaliar algumas opções de arquitetura e ver qual seria o melhor custo benefício. As arquiteturas testadas podem ser visualizadas na tabela \ref{tab:arqs}. Importante salientar que todas as arquiteturas possuem apenas um nó Mestre e os slaves sempre têm a mesma configuração.

Houveram problemas de alocação de arquitetura, pois há restrições no plano free da \emph{Google Cloud}. Conseguimos montar um cluster com no máximo 10 vCPUs e com no máximo 100GB (somatório total) de SSD.

A avaliação foi realizada inicialmente, apenas carregando os dados, com o trecho de código abaixo. Para cada execução, foi reiniciado o Kernel Python.

\begin{lstlisting}[caption= {Carga de dados no Spark},captionpos=b, language=python]
start = time.time()

df = sqlContext.read.option("delimiter", ';')
.load('hdfs:///user/aksmiyazaki/enem_2016.csv', 
                      format='com.databricks.spark.csv', 
                      header='true', 
                      inferSchema='true')

end = time.time()
t = end - start
\end{lstlisting}


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Identificador & Número de Slaves & vCPUs &  Memória & Disco \\ \hline \hline
1 & 2 & 2 & 7,5 GB & 200GB HD \\ \hline
2 & 2 & 2 & 7.5 GB & 30GB SSD \\ \hline
3 & 3 & 2 & 7.5 GB & 20GB SSD\\ \hline
\end{tabular}
\caption{Descrição de Arquiteturas testadas.}
\label{tab:arqs}
\end{table}

Após a carga, foi criada uma \emph{lazily evaluated view} (trecho de código abaixo). Essa tabela pode ser consultada com SparkSQL. 

\begin{lstlisting}[caption= {Criação da Tabela para Consulta},captionpos=b, language=python]
df.createOrReplaceTempView("table_enem2016")
\end{lstlisting}

Com a tabela criada, consultamos os dados conforme código abaixo. É importante salientar que nesse ponto o Kernel não é mais reinicializado.

\begin{lstlisting}[caption= {Consulta Simplificada para Avaliação},captionpos=b, language=python]
start = time.time()

df2 = spark.sql("Select mean(NU_NOTA_CN) from table_enem2016")
df2.collect()

end = time.time()
\end{lstlisting}

Os resultados da avaliação (média de tempos) podem ser visualizados na Tabela \ref{tab:mean_times}. Para maiores detalhes, consulte a Seção \ref{sect:eval_data}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Configuração & Carga de Dados (média) &  Execução de  Consulta Simples (média) \\ \hline 
\hline
1 & 184.79 &  57.83
\\ \hline
2 & 172.98 & 49.72
 \\ \hline
3 & 124.81
 &  40.24
 \\ \hline
\end{tabular}
\caption{Tempos médios de execução, em \textbf{segundos}.}
\label{tab:mean_times}
\end{table}

Com base na análise desses tempos, segui utilizando a configuração número 3. Isso porque, ela foi a que apresentou melhores resultados em termos de desempenho, e pode nos dar maior agilidade na avaliação exploratória desses dados. É importante salientar que essa configuração pode não ser o melhor custo benefício (isso precisaria ser levado em consideração no mundo real).

\newpage
\section{Análise Exploratória dos Dados}

A análise dos dados tem como principal objetivo identificar \emph{Features} que tenham relação com as notas dos candidatos. Para isso, irei realizando inferências a medida que a análise avançar, exibindo os resultados obtidos.

A primeira intuição é avaliar se as regiões do País afetam as médias dos candidatos. Para isso, executou-se uma consulta , que calculava também a média total, gerando o gráfico da Figura \ref{fig:avg_uf}.

\begin{figure}[H]
\centering
  \includegraphics[width= 0.8 \linewidth]{img/mean_by_uf.png}
  \caption{Médias por UF.}
  \label{fig:fig_maq_criad}
\end{figure}

É visível a discrepância de notas da parte sudeste do país com o restante. Portanto, concluí-se que essa é uma \ref{Feature} bem significativa para inferir a nota do candidato.

Fazendo uma análise da idade relacionada a média total, podemos ver uma tendencia decrescente nos dados a medida que a idade aumenta, na Figura \ref{fig:mean_by_age}. 

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/mean_by_age.png}
  \caption{Médias por Idade.}
  \label{fig:mean_by_age}
\end{figure}

Também foi realizada uma análise, verificando a renda familiar (informada na Feature Q006). A renda é representada por letras, onde A indica sem renda e Q indica mais de R\$ 17.600,00 (ordem crescente). Na Figura \ref{fig:mean_by_wage}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/mean_by_wage.png}
  \caption{Médias por Faixa de Renda.}
  \label{fig:mean_by_wage}
\end{figure}

\newpage
\section*{Apêndice}
\label{sect:eval_data}
\subsection*{Execução de consultas por Arquitetura}

É importante notar que os identificadores citados nas Tabelas \ref{tab:times_load} e \ref{tab:query} se referem àqueles citados na Tabela \ref{tab:arqs}. A abordagem de avaliação foi bem simplificada, tendo em vista que fazer uma real análise de desempenho não faz parte do escopo do trabalho.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Identificador da Arquitetura & T1 & T2 & T3\\ \hline 
\hline
1 &  187.68613958358765 & 178.42478108406067 & 188.2514045238495\\ \hline
2 & 172.11613035202026 & 172.5200114250183 & 174.2983114719391 \\ \hline
3 & 127.42551612854
 & 121.673460960388
& 125.33324432373
\\ \hline
\end{tabular}
\caption{Tempos de Carga, em \textbf{segundos}.}
\label{tab:times_load}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Identificador da Arquitetura & T1 & T2 & T3\\ \hline 
\hline
1 & 63.009117603302 & 61.09515404701233 & 49.37240242958069\\ \hline
2 & 58.08870315551758 & 46.74635648727417 & 44.3299551010132
\\ \hline
3 & 43.8593015670776
 & 44.337733745575
& 32.537876367569
\\ \hline
\end{tabular}
\caption{Tempos de Consulta, em \textbf{segundos}.}
\label{tab:query}
\end{table}



\end{document}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/machines_created.png}
  \caption{Máquinas criadas na \emph{cloud}.}
  \label{fig:fig_maq_criad}
\end{figure}

